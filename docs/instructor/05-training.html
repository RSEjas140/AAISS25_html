<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><title>AISS25: Machine learning: model training</title><meta name="viewport" content="width=device-width, initial-scale=1"><script src="../assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="../assets/styles.css"><script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../favicons/swc/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="../favicons/swc/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="../favicons/swc/favicon-16x16.png"><link rel="manifest" href="../favicons/swc/site.webmanifest"><link rel="mask-icon" href="../favicons/swc/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#da532c"><meta name="theme-color" media="(prefers-color-scheme: light)" content="white"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="black"></head><body>
    <header id="top" class="navbar navbar-expand-md top-nav software"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="swc-logo" alt="Aberystwyth AI Summer School 2025" src="../assets/images/software-logo.svg"><span class="badge text-bg-warning">
          <abbr title="This lesson is in the alpha phase, which means that it has been taught once and lesson authors are iterating on feedback.">
            <a href="https://docs.carpentries.org/resources/curriculum/lesson-life-cycle.html" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-triangle" style="border-radius: 5px"></i>
              Alpha
            </a>
            <span class="visually-hidden">This lesson is in the alpha phase, which means that it has been taught once and lesson authors are iterating on feedback.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text"><li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul></li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1"><li><button class="dropdown-item" type="button" onclick="window.location.href='../05-training.html';">Learner View</button></li>
        </ul></div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav software" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Aberystwyth AI Summer School 2025" src="../assets/images/software-logo-sm.svg"></div>
    <div class="lesson-title-md">
      AISS25
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0"><li class="nav-item">
          <span class="lesson-title">
            AISS25
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown"><hr><li><a class="dropdown-item" href="reference.html">Reference</a></li>
          </ul></li>
      </ul></div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="../instructor/aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div><!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  AISS25
</div>

<aside class="col-md-12 lesson-progress"><div style="width: 100%" class="percentage">
    100%
  </div>
  <div class="progress software">
    <div class="progress-bar software" role="progressbar" style="width: 100%" aria-valuenow="100" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text"><li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul></li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../05-training.html">Learner View</a>
                      </div>
                    </div>
                  </div><!--/div.accordion-item-->
                </div><!--/div.accordion-flush-->
              </div><!--div.sidenav-view-selector -->
            </div><!--/div.col -->

            <hr></div><!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="00-worked-example.html">1. Working Example</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="01-problem-definition.html">2. Problem Definition</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="02-data-exploration-and-preprocessing.html">3. Data exploration and Preprocessing</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="04-model-selection%202.html">4. Machine learning: Model Selection</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlushcurrent">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-headingcurrent">
      <button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapsecurrent" aria-expanded="true" aria-controls="flush-collapsecurrent">
        <span class="visually-hidden">Current Chapter</span>
        <span class="current-chapter">
        5. Machine learning: model training
        </span>
      </button>
    </div><!--/div.accordion-header-->

    <div id="flush-collapsecurrent" class="accordion-collapse collapse show" aria-labelledby="flush-headingcurrent" data-bs-parent="#accordionFlushcurrent">
      <div class="accordion-body">
        <ul><li><a href="#training-machine-learning-models">Training Machine learning models</a></li>
<li><a href="#dataset-configuration-1">Dataset configuration</a></li>
<li><a href="#sampling-methods">Sampling methods</a></li>
<li><a href="#what-are-class-weights">What Are Class Weights?</a></li>
<li><a href="#cross-validation">Cross validation</a></li>
<li><a href="#loss-function">Loss function</a></li>
<li><a href="#analysing-loss-curves">Analysing loss curves</a></li>
<li><a href="#what-is-hyperparameter-tuning">What is Hyperparameter Tuning?</a></li>
<li><a href="#putting-it-all-together-examples">Putting it all together examples</a></li>
<li><a href="#classification-example">Classification example</a></li>
<li><a href="#time-series-parameter-example">Time series parameter example</a></li>
<li><a href="#parameter-estimation-example">Parameter estimation example</a></li>
        </ul></div><!--/div.accordion-body-->
    </div><!--/div.accordion-collapse-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="06-model-evaluation.html">6. Model Evaluation</a>
    </div><!--/div.accordion-header-->

  </div><!--/div.accordion-item-->
</div><!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width"><div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul><li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <hr><li><a class="dropdown-item" href="reference.html">Reference</a></li>
                    </ul></div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources"><a href="../instructor/aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none"><div class="d-grid gap-1">

            </div>
          </div><!-- /div.accordion -->
        </div><!-- /div.sidebar-inner -->
      </nav></div><!-- /div.sidebar -->
  </div><!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-instructor.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <nav class="lesson-content mx-md-4" aria-label="Previous and Next Chapter"><!-- content for small screens --><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/04-model-selection%202.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/06-model-evaluation.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/04-model-selection%202.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Machine learning:
        </a>
        <a class="chapter-link float-end" href="../instructor/06-model-evaluation.html" rel="next">
          Next: Model Evaluation...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
      <hr></nav><main id="main-content" class="main-content"><div class="container lesson-content">
        <h1>Machine learning: model training</h1>
        <p>Last updated on 2025-07-09 |

        <a href="https://github.com/RSEjas140/AAISS25_html/edit/main/episodes/05-training.Rmd" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>



        <p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 46 minutes</p>

        <div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>

        

<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul><li>What is required to train a machine learning model?</li>
<li>How do I deal with bias in my dataset?</li>
<li>How do I know if my model is training correctly?</li>
<li>What are hyper parameters?</li>
</ul></div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul><li>To learn all the aspect required to train a machine learning
model.</li>
<li>To use dataset configurations to deal with dataset bias.</li>
<li>Learn about cross-validation training.</li>
<li>To understand all the different hyper parameters and how to decided
what configurations to select.</li>
</ul></div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="training-machine-learning-models">Training Machine learning models<a class="anchor" aria-label="anchor" href="#training-machine-learning-models"></a></h2>
<hr class="half-width"><p>Training a machine learning model is often considered the most
difficult part for several reasons, many of which go beyond just running
code. So far, we have covered the following:</p>
<p><br><br></p>
<div class="section level3">
<h3 id="data-quality-preparation-the-biggest-bottleneck">Data Quality &amp; Preparation (The Biggest Bottleneck)<a class="anchor" aria-label="anchor" href="#data-quality-preparation-the-biggest-bottleneck"></a></h3>
<ul><li><p>Garbage In, Garbage Out: No matter how advanced your model, if
the data is incomplete, noisy, biased, or inconsistent, the model will
perform poorly.</p></li>
<li><p>Data Cleaning: Real-world data is messy. Handling missing values,
duplicates, incorrect labels, or outliers is tedious and
time-consuming.</p></li>
<li><p>Feature Engineering: Identifying which aspects of the data are
relevant requires domain expertise and creativity. It’s often more
impactful than tweaking algorithms.</p></li>
</ul><p><br><br></p>
</div>
<div class="section level3">
<h3 id="selecting-the-right-model">Selecting the Right Model<a class="anchor" aria-label="anchor" href="#selecting-the-right-model"></a></h3>
<ul><li>There’s no one-size-fits-all model. Choosing between decision trees,
neural networks, support vector machines, etc.</li>
<li>Understanding the benefit of certain model types compared to
others.</li>
</ul><p><br><br></p>
<div id="lets-look-at-the-distrabution" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="lets-look-at-the-distrabution" class="callout-inner">
<h3 class="callout-title">Lets look at the distrabution</h3>
<div class="callout-content">
<p>Q: Which type of model are you going to focus on
(supervised/unsupervised)</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>A: results on board</p>
</div>
</div>
</div>
</div>
<p><br></p>
<p>Now in this section we are going to cover the actual process of
training with more complex machine learning methods like deep learning
and what aspects to pay attention to too. These include:</p>
<p><br><br></p>
</div>
<div class="section level3">
<h3 id="dataset-configuration">Dataset configuration<a class="anchor" aria-label="anchor" href="#dataset-configuration"></a></h3>
<ul><li><p>Evaluate the level of bias in your dataset (sampling methods, or
class wieghts).</p></li>
<li><p>Constructing a reproducible training method (cross
validation).</p></li>
</ul><p><br><br></p>
</div>
<div class="section level3">
<h3 id="loss-functions-and-curves">Loss functions and curves<a class="anchor" aria-label="anchor" href="#loss-functions-and-curves"></a></h3>
<ul><li>Striking the balance between over fitting and underfitting, which
requires constant testing, validation, and sometimes redesigning the
model or features.</li>
</ul><p><br><br></p>
</div>
<div class="section level3">
<h3 id="hyperparameter-tuning">Hyperparameter Tuning<a class="anchor" aria-label="anchor" href="#hyperparameter-tuning"></a></h3>
<ul><li>Most models have hyperparameters (e.g., learning rate, number of
layers) that significantly affect performance.</li>
<li>How to find the most optimal configuration (Trial and error, Grid
search)</li>
<li>This can be computationally expensive and time-consuming. <br><br></li>
</ul></div>
<div class="section level3">
<h3 id="compute-resources-scalability">Compute Resources &amp; Scalability<a class="anchor" aria-label="anchor" href="#compute-resources-scalability"></a></h3>
<ul><li>Especially with deep learning or large datasets, training requires
powerful GPUs/TPUs, distributed computing, or cloud resources.</li>
<li>Managing these resources efficiently adds another layer of
complexity.</li>
</ul><p><br><br></p>
<p>Training is where all the complexity of creating a machine learning
model converges: technical, mathematical, computational, and
domain-specific. It’s not just about applying an algorithm; it’s about
understanding the nuances of your data, selecting the right tools,
balancing performance, and iterating continuously.</p>
<p><br></p>
<div id="hpc-resources" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="hpc-resources" class="callout-inner">
<h3 class="callout-title">HPC resources</h3>
<div class="callout-content">
<p>Q: Do you know of any HPC resources available to you? and when should
you consider using them?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2"> Show me the solution </h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>A: aberystwyth own HPC, Super computing wales. You should consider
using these when you code takes reasonable amount of time, the good
thing is these systems are scalable.</p>
</div>
</div>
</div>
</div>
<p><br><br></p>
</div>
</section><section><h2 class="section-heading" id="dataset-configuration-1">Dataset configuration<a class="anchor" aria-label="anchor" href="#dataset-configuration-1"></a></h2>
<hr class="half-width"><p>The first thing we need to consider is whether there’s any bias
present in our training data. For discrete data, this could mean that
one class is significantly more represented than others. In the case of
continuous data, it might involve most data points being concentrated
within a specific range of values.</p>
<p>These types of imbalances are easy to check. For discrete data, you
can simply count the number of samples in each class. For continuous
data, plotting a histogram of your target variable will reveal any
uneven distributions.</p>
<p>You might wonder why this matters and, in some cases, it’s not a
major issue. However, for certain approaches like deep learning, models
often develop a bias toward the most common class, leading them to
predict that class most of the time, regardless of the input.</p>
<p>An analogy for this is imagining a river flowing down a mountain.
When the river meets a fork, most of the water will naturally flow down
the wider or easier path following the path of least resistance.
Similarly, if our data is imbalanced, the model tends to take the
“easiest” option by favouring the dominant class, rather than truly
learning to distinguish between different classes based on meaningful
patterns.</p>
<p>To build a more balanced and well-rounded model, we need to correct
for these imbalances. Two common strategies are:</p>
<ul><li>Resampling the Data — either by oversampling minority classes or
under sampling the majority class</li>
<li>Applying Class Weights — adjusting the model to penalize incorrect
predictions more heavily for underrepresented classes</li>
</ul><p>Both approaches aim to encourage the model to use the actual features
for prediction, rather than being influenced by biased distributions in
the training data.</p>
</section><section><h2 class="section-heading" id="sampling-methods">Sampling methods<a class="anchor" aria-label="anchor" href="#sampling-methods"></a></h2>
<hr class="half-width"><p>sampling methods in machine learning, which are essential for
creating balanced, representative datasets, especially in classification
problems, imbalanced datasets, or evaluation tasks like
cross-validation.</p>
<p>Types of Sampling Methods in Machine Learning</p>
<ul><li>Random Sampling: Selects data points randomly from the dataset.</li>
<li>Stratified Sampling: Ensures each class is proportionally
represented in the sample.</li>
<li>Systematic Sampling: Selects every k-th data point from the
dataset.</li>
<li>Cluster Sampling: Divides the dataset into clusters and randomly
selects some clusters.</li>
<li>Reservoir Sampling: Efficient method to sample k items from a very
large or streaming dataset.</li>
<li>Over-Sampling: Increases the number of samples in the minority
class.</li>
<li>Under-Sampling: Reduces the number of samples in the majority
class.</li>
<li>Bootstrapping: Samples with replacement. Used often in ensemble
methods like bagging.</li>
<li>Cross-Validation Sampling: Splits the data into multiple folds to
validate models reliably.</li>
</ul><p>Each method has its own benefit and disadvantages, therefore it’s
important to do a lot of testing to work out which is the most optimal
for your data type. The most common and is usually effective no matter
what your data type is Under-Sampling. The main downside of this method
is that you are removing samples, which means that you are restricting
your model’s knowledge base. Let’s move on to our method, which doesn’t
involve removing any data.</p>
<p><img src="../fig/probability-sampling.png" alt="Flow Diagram for determining supvervised vs unsupervised" class="figure">.</p>
<p>(cite: <a href="https://www.scribbr.co.uk/research-methods/sampling/" class="external-link uri">https://www.scribbr.co.uk/research-methods/sampling/</a>)</p>
<div class="section level3">
<h3 id="stratified-sampling-example">Stratified Sampling example<a class="anchor" aria-label="anchor" href="#stratified-sampling-example"></a></h3>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> datasets</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>iris <span class="op">=</span> datasets.load_iris()</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>iris_df<span class="op">=</span>pd.DataFrame(iris.data)</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>iris_df[<span class="st">'class'</span>]<span class="op">=</span>iris.target</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>iris_df.columns<span class="op">=</span>[<span class="st">'sepal_len'</span>, <span class="st">'sepal_wid'</span>, <span class="st">'petal_len'</span>, <span class="st">'petal_wid'</span>, <span class="st">'class'</span>]</span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>iris_df[<span class="st">'class'</span>].value_counts()</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>y <span class="op">=</span> iris_df[<span class="st">"class"</span>]</span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>X <span class="op">=</span> iris_df[iris_df.columns]</span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a>X_train, X_test, y_train, y_test<span class="op">=</span> train_test_split(X,y, train_size<span class="op">=</span><span class="fl">0.8</span>, random_state<span class="op">=</span><span class="va">None</span>,shuffle<span class="op">=</span><span class="va">True</span>, stratify<span class="op">=</span>y)</span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Class distribution of train set"</span>)</span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a><span class="bu">print</span>(y_train.value_counts())</span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Class distribution of test set"</span>)</span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a><span class="bu">print</span>(y_test.value_counts())</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Class distribution of train set
class
0    40
2    40
1    40
Name: count, dtype: int64

Class distribution of test set
class
1    10
2    10
0    10
Name: count, dtype: int64</code></pre>
</div>
</div>
</section><section><h2 class="section-heading" id="what-are-class-weights">What Are Class Weights?<a class="anchor" aria-label="anchor" href="#what-are-class-weights"></a></h2>
<hr class="half-width"><p>Class weights are numerical values assigned to different classes
during model training to handle class imbalance, where one or more
classes are significantly underrepresented compared to others.</p>
<p>In simple terms:</p>
<ul><li>The model is “told” that some classes are more important or costly
to miss-classify.</li>
<li>It increases the penalty for incorrect predictions on minority
classes.</li>
<li>This prevents the model from being biased toward the dominant
class.</li>
</ul><div class="section level3">
<h3 id="why-use-class-weights">Why Use Class Weights?<a class="anchor" aria-label="anchor" href="#why-use-class-weights"></a></h3>
<p>In imbalanced datasets, if no adjustments are made:</p>
<ul><li>A model might predict the majority class most of the time and still
appear to perform well (e.g., 95% accuracy might just mean it’s always
predicting the dominant class).</li>
<li>Minority classes get neglected, which is a problem for tasks like
fraud detection, medical diagnosis, etc.</li>
</ul><p>Class weights help by:</p>
<ul><li>Encouraging the model to “pay more attention” to rare classes</li>
<li>Reducing bias toward the majority class</li>
<li>Improving recall and precision for minority classes</li>
</ul></div>
<div class="section level3">
<h3 id="how-class-weights-work-conceptually">How Class Weights Work (Conceptually)<a class="anchor" aria-label="anchor" href="#how-class-weights-work-conceptually"></a></h3>
<p>During training:</p>
<ul><li>The loss function (more on the loss function later) calculates how
wrong the model is.</li>
<li>Class weights scale the loss differently for each class:</li>
<li>Misclassifying a minority class → Higher penalty</li>
<li>Misclassifying a majority class → Lower penalty</li>
</ul><p>This nudges the optimization process to treat minority classes more
seriously.</p>
</div>
<div class="section level3">
<h3 id="example">Example<a class="anchor" aria-label="anchor" href="#example"></a></h3>
<p>Imagine a binary classification problem:</p>
<pre><code>Class 0 = 90% of data

Class 1 = 10% of data </code></pre>
<p>Without class weights:</p>
<pre><code>The model might always predict Class 0 to get high accuracy. </code></pre>
<p>With class weights:</p>
<pre><code>Assign higher weight to Class 1: </code></pre>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a><span class="im">from</span> sklearn.utils.class_weight <span class="im">import</span> compute_class_weight</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>y <span class="op">=</span> [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>compute_class_weight(class_weight<span class="op">=</span><span class="st">"balanced"</span>, classes<span class="op">=</span>np.unique(y), y<span class="op">=</span>y)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Out[14]: array([1.5 , 0.75])</code></pre>
</div>
</div>
</section><section><h2 class="section-heading" id="cross-validation">Cross validation<a class="anchor" aria-label="anchor" href="#cross-validation"></a></h2>
<hr class="half-width"><p>Cross-validation is a technique used to evaluate the performance of a
machine learning model by splitting the data into multiple parts
(folds), training on some and validating on the rest repeatedly. This
helps ensure the model generalizes well to unseen data.</p>
<div class="section level3">
<h3 id="why-use-cross-validation">Why Use Cross-Validation?<a class="anchor" aria-label="anchor" href="#why-use-cross-validation"></a></h3>
<ul><li>Prevents overfitting to a single train/test split.</li>
<li>Gives a better estimate of model performance.</li>
<li>Works especially well with limited datasets.</li>
</ul></div>
<div class="section level3">
<h3 id="common-types-of-cross-validation">Common Types of Cross-Validation<a class="anchor" aria-label="anchor" href="#common-types-of-cross-validation"></a></h3>
<ul><li>K-Fold Cross-Validation: Splits data into K equal folds.</li>
<li>Stratified K-Fold: Like K-Fold but preserves class proportions in
each fold.</li>
<li>Leave-One-Out (LOO): Extreme case of K-Fold where K = N (number of
samples).</li>
<li>Repeated K-Fold: Repeats K-Fold CV multiple times with different
splits.</li>
</ul><p><img src="../fig/cross_validation.png" alt="Flow Diagram for determining supvervised vs unsupervised" class="figure">.</p>
<p>(cite: <a href="https://vitalflux.com/k-fold-cross-validation-python-example/" class="external-link uri">https://vitalflux.com/k-fold-cross-validation-python-example/</a>)</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> datasets</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a><span class="co"># synthetic regression dataset</span></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a>X, y <span class="op">=</span> datasets.make_regression(</span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a>  n_samples<span class="op">=</span><span class="dv">10</span>, n_features<span class="op">=</span><span class="dv">1</span>, n_informative<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a>  noise<span class="op">=</span><span class="dv">0</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a><span class="co"># KFold split</span></span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>kf <span class="op">=</span> KFold(n_splits<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a><span class="cf">for</span> i, (train_index, test_index) <span class="kw">in</span> <span class="bu">enumerate</span>(kf.split(X)):</span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Fold </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb8-14"><a href="#cb8-14" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Training dataset index: </span><span class="sc">{</span>train_index<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-15"><a href="#cb8-15" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Test dataset index: </span><span class="sc">{</span>test_index<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Fold 0:
  Training dataset index: [3 4 5 6 7 8 9]
  Test dataset index: [0 1 2]
Fold 1:
  Training dataset index: [0 1 2 6 7 8 9]
  Test dataset index: [3 4 5]
Fold 2:
  Training dataset index: [0 1 2 3 4 5 8 9]
  Test dataset index: [6 7]
Fold 3:
  Training dataset index: [0 1 2 3 4 5 6 7]
  Test dataset index: [8 9]</code></pre>
</div>
</div>
</section><section><h2 class="section-heading" id="loss-function">Loss function<a class="anchor" aria-label="anchor" href="#loss-function"></a></h2>
<hr class="half-width"><div class="section level3">
<h3 id="what-is-a-loss-function-in-machine-learning">What is a Loss Function in Machine Learning?<a class="anchor" aria-label="anchor" href="#what-is-a-loss-function-in-machine-learning"></a></h3>
<p>A loss function (also called a cost function) is a mathematical
function that measures the difference between the predicted output and
the actual target value. It tells the model how wrong it is and is
essential for the learning process and is variable tending on the type
of task the model is performing. With the goal of training is to
minimize the loss function.</p>
</div>
<div class="section level3">
<h3 id="why-is-it-important">Why Is It Important?<a class="anchor" aria-label="anchor" href="#why-is-it-important"></a></h3>
<ul><li>Guides the model during training (via optimization, e.g., gradient
descent).</li>
<li>Helps compare performance between different models.</li>
<li>Lower loss = better model (usually).</li>
</ul></div>
<div class="section level3">
<h3 id="common-types-of-loss-functions">Common Types of Loss Functions<a class="anchor" aria-label="anchor" href="#common-types-of-loss-functions"></a></h3>
</div>
<div class="section level3">
<h3 id="for-regression">For Regression<a class="anchor" aria-label="anchor" href="#for-regression"></a></h3>
<p>MAE (Mean Absolute Error) and MSE (Mean Squared Error) are two
commonly used metrics to evaluate the performance of regression models
by measuring the difference between predicted values and actual
values.</p>
</div>
<div class="section level3">
<h3 id="mean-absolute-error-mae">Mean Absolute Error (MAE)<a class="anchor" aria-label="anchor" href="#mean-absolute-error-mae"></a></h3>
<p>Definition: MAE is the average of the absolute differences between
predicted values and actual values.</p>
<p>Formula:</p>
<p><span class="math inline">\(MAE = \frac{1}{n}\sum_{i = 1}^{n} ∣y_i −
x_i∣\)</span></p>
<p>Where:</p>
<ul><li>
<span class="math inline">\(n\)</span> = number of data points</li>
<li>
<span class="math inline">\(x_i\)</span>= actual value</li>
<li>
<span class="math inline">\(y_i\)</span> = predicted value</li>
</ul><div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_absolute_error</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>y_true <span class="op">=</span> [<span class="dv">3</span>, <span class="op">-</span><span class="fl">0.5</span>, <span class="dv">2</span>, <span class="dv">7</span>]</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a>y_pred <span class="op">=</span> [<span class="fl">2.5</span>, <span class="fl">0.0</span>, <span class="dv">2</span>, <span class="dv">8</span>]</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a>mean_absolute_error(y_true, y_pred)</span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>y_true <span class="op">=</span> [[<span class="fl">0.5</span>, <span class="dv">1</span>], [<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>], [<span class="dv">7</span>, <span class="op">-</span><span class="dv">6</span>]]</span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a>y_pred <span class="op">=</span> [[<span class="dv">0</span>, <span class="dv">2</span>], [<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">8</span>, <span class="op">-</span><span class="dv">5</span>]]</span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a>mean_absolute_error(y_true, y_pred)</span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a>mean_absolute_error(y_true, y_pred, multioutput<span class="op">=</span><span class="st">'raw_values'</span>)</span>
<span id="cb10-10"><a href="#cb10-10" tabindex="-1"></a>mean_absolute_error(y_true, y_pred, multioutput<span class="op">=</span>[<span class="fl">0.3</span>, <span class="fl">0.7</span>])</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="va">Out</span><span class="op">[</span><span class="fl">26</span><span class="op">]</span><span class="op">:</span> <span class="fl">0.85</span></span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="mean-squared-error-mse">Mean Squared Error (MSE)<a class="anchor" aria-label="anchor" href="#mean-squared-error-mse"></a></h3>
<p>Definition: MSE is the average of the squared differences between
predicted values and actual values.</p>
<p>Formula: <span class="math inline">\(MAE = \frac{1}{n}\sum_{i =
1}^{n}(y_i − x_i)^2\)</span> Where:</p>
<ul><li>
<span class="math inline">\(n\)</span> = number of data points</li>
<li>
<span class="math inline">\(x_i\)</span>= actual value</li>
<li>
<span class="math inline">\(y_i\)</span> = predicted value</li>
</ul><div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>y_true <span class="op">=</span> [<span class="dv">3</span>, <span class="op">-</span><span class="fl">0.5</span>, <span class="dv">2</span>, <span class="dv">7</span>]</span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a>y_pred <span class="op">=</span> [<span class="fl">2.5</span>, <span class="fl">0.0</span>, <span class="dv">2</span>, <span class="dv">8</span>]</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>mean_squared_error(y_true, y_pred)</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>y_true <span class="op">=</span> [[<span class="fl">0.5</span>, <span class="dv">1</span>],[<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>],[<span class="dv">7</span>, <span class="op">-</span><span class="dv">6</span>]]</span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a>y_pred <span class="op">=</span> [[<span class="dv">0</span>, <span class="dv">2</span>],[<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>],[<span class="dv">8</span>, <span class="op">-</span><span class="dv">5</span>]]</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a>mean_squared_error(y_true, y_pred)</span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a>mean_squared_error(y_true, y_pred, multioutput<span class="op">=</span><span class="st">'raw_values'</span>)</span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a>mean_squared_error(y_true, y_pred, multioutput<span class="op">=</span>[<span class="fl">0.3</span>, <span class="fl">0.7</span>])</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="va">Out</span><span class="op">[</span><span class="fl">28</span><span class="op">]</span><span class="op">:</span> <span class="fl">0.825</span></span></code></pre>
</div>
<p>In summary,</p>
<ul><li>Use MAE if you want a metric that is more interpretable and less
affected by outliers.</li>
<li>Use MSE (or RMSE, its square root) if large errors are especially
undesirable and need stronger penalization.</li>
</ul><p><img src="../fig/mse.png" alt="Flow Diagram for determining supvervised vs unsupervised" class="figure">.</p>
</div>
<div class="section level3">
<h3 id="for-classification">For Classification<a class="anchor" aria-label="anchor" href="#for-classification"></a></h3>
</div>
<div class="section level3">
<h3 id="binary-crossentropy-log-loss">Binary Crossentropy (Log Loss)<a class="anchor" aria-label="anchor" href="#binary-crossentropy-log-loss"></a></h3>
<p>When to Use: - For binary classification problems (two classes: 0 or
1) - Also works for multi-label classification where each label is
treated independently</p>
<p>Formula:</p>
<p>For a single data point:</p>
<p>L=<span class="math inline">\(−(y*log(y^i)+(1−y)*log(1−y^i))\)</span></p>
<p>Where:</p>
<ul><li>
<span class="math inline">\(y=\)</span> actual label (0 or 1)</li>
<li>
<span class="math inline">\(y^i=\)</span> predicted probability for
class 1 (between 0 and 1)</li>
</ul><p>Example Use Case:</p>
<ul><li>Spam detection (spam = 1, not spam = 0)</li>
<li>Predicting disease presence (positive/negative)</li>
</ul><p>Example:</p>
<p>Let’s say you have two data points:</p>
<ul><li>Point 1: True label (y) = 1, predicted probability (p) = 0.8</li>
<li>Point 2: True label (y) = 0, predicted probability (p) = 0.3</li>
<li>Point 1 loss: -(1 * log(0.8) + (1 - 1) * log(1 - 0.8)) = -log(0.8) ≈
0.223</li>
<li>Point 2 loss: -(0 * log(0.3) + (1 - 0) * log(1 - 0.3)) = -log(0.7) ≈
0.357</li>
</ul><p>Total loss: 0.223 + 0.357 = 0.58 Average loss: 0.58 / 2 = 0.29</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> log_loss</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>log_loss([<span class="st">"spam"</span>, <span class="st">"ham"</span>, <span class="st">"ham"</span>, <span class="st">"spam"</span>],[[<span class="fl">.1</span>, <span class="fl">.9</span>], [<span class="fl">.9</span>, <span class="fl">.1</span>], [<span class="fl">.8</span>, <span class="fl">.2</span>], [<span class="fl">.35</span>, <span class="fl">.65</span>]])</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="fl">0.21616</span></span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="categorical-crossentropy">Categorical Crossentropy<a class="anchor" aria-label="anchor" href="#categorical-crossentropy"></a></h3>
<p>When to Use: - For multi-class classification problems (more than 2
classes) - Labels are one-hot encoded (e.g., for 3 classes: [1, 0, 0],
[0, 1, 0], [0, 0, 1])</p>
<p>Formula:</p>
<p>L<span class="math inline">\(=−c\sum_{i = 1}^{n} yc
⋅log(y^c)\)</span></p>
<p>Where:</p>
<ul><li>
<span class="math inline">\(C=\)</span> number of classes</li>
<li>
<span class="math inline">\(yc=\)</span> actual label (1 for correct
class, 0 otherwise)</li>
<li>
<span class="math inline">\(y^c =\)</span> predicted probability for
class c (output of softmax)</li>
</ul><p>Example Use Case:</p>
<ul><li>Image classification with multiple classes (e.g., cats, dogs,
horses)</li>
<li>Sentiment analysis with multiple categories (positive, neutral,
negative)</li>
</ul><div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a><span class="co"># Example of target with class indices</span></span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>loss <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">5</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a>target <span class="op">=</span> torch.empty(<span class="dv">3</span>, dtype<span class="op">=</span>torch.<span class="bu">long</span>).random_(<span class="dv">5</span>)</span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" tabindex="-1"></a><span class="bu">print</span>(target)</span>
<span id="cb16-10"><a href="#cb16-10" tabindex="-1"></a>output <span class="op">=</span> loss(<span class="bu">input</span>, target)</span>
<span id="cb16-11"><a href="#cb16-11" tabindex="-1"></a><span class="bu">print</span>(output)</span>
<span id="cb16-12"><a href="#cb16-12" tabindex="-1"></a>output.backward()</span>
<span id="cb16-13"><a href="#cb16-13" tabindex="-1"></a><span class="co"># Example of target with class probabilities</span></span>
<span id="cb16-14"><a href="#cb16-14" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">5</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-15"><a href="#cb16-15" tabindex="-1"></a>target <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">5</span>).softmax(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-16"><a href="#cb16-16" tabindex="-1"></a>output <span class="op">=</span> loss(<span class="bu">input</span>, target)</span>
<span id="cb16-17"><a href="#cb16-17" tabindex="-1"></a>output.backward()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>tensor([0, 1, 4])
tensor(1.5236, grad_fn=&lt;NllLossBackward0&gt;)
</code></pre>
</div>
<p><img src="../fig/differences.png" alt="Flow Diagram for determining supvervised vs unsupervised" class="figure">.</p>
<p>Summary</p>
<ul><li>Use Binary Crossentropy for 2-class problems or independent
multi-label outputs</li>
<li>Use Categorical Crossentropy for multi-class, one-hot encoded
labels</li>
<li>Use Sparse Categorical Crossentropy for multi-class problems with
integer labels</li>
</ul></div>
</section><section><h2 class="section-heading" id="analysing-loss-curves">Analysing loss curves<a class="anchor" aria-label="anchor" href="#analysing-loss-curves"></a></h2>
<hr class="half-width"><div class="section level3">
<h3 id="what-are-loss-curves">What Are Loss Curves?<a class="anchor" aria-label="anchor" href="#what-are-loss-curves"></a></h3>
<p>Loss curves plot the model’s loss over time (typically per epoch)
for:</p>
<ul><li>Training Loss: How well the model fits the training data</li>
<li>Validation Loss: How well the model generalizes to unseen data</li>
</ul><p>Goal:</p>
<ul><li>Steady decrease in training and validation loss, with minimal gap
between them to avoid overfitting</li>
</ul></div>
<div class="section level3">
<h3 id="how-to-interpret-loss-curves">How to Interpret Loss Curves<a class="anchor" aria-label="anchor" href="#how-to-interpret-loss-curves"></a></h3>
</div>
<div class="section level3">
<h3 id="ideal-scenario">Ideal Scenario<a class="anchor" aria-label="anchor" href="#ideal-scenario"></a></h3>
<ul><li>Both training and validation loss decrease smoothly Validation loss
stabilizes or slightly increases at the end</li>
<li>Model is learning and generalizing well</li>
</ul><p><img src="../fig/864224b4224e47216fdf443996b517f9e259438d.png" alt="Flow Diagram for determining supvervised vs unsupervised" class="figure">.</p>
<p>(cite: <a href="https://discuss.pytorch.org/t/plotting-loss-curve/42632" class="external-link uri">https://discuss.pytorch.org/t/plotting-loss-curve/42632</a>)</p>
<p><img src="../fig/images.jpeg" alt="Flow Diagram for determining supvervised vs unsupervised" class="figure">.</p>
<p>(cite:<a href="https://www.digilab.ai/" class="external-link uri">https://www.digilab.ai/</a>)</p>
</div>
<div class="section level3">
<h3 id="overfitting">Overfitting<a class="anchor" aria-label="anchor" href="#overfitting"></a></h3>
<ul><li>Training loss keeps decreasing</li>
<li>Validation loss decreases initially, then starts increasing</li>
</ul><p>Symptoms: - High accuracy on training data - Poor performance on
validation/test data</p>
<p>Solutions:</p>
<ul><li>Early stopping</li>
<li>Add dropout or regularization</li>
<li>Reduce model complexity</li>
<li>Data augmentation</li>
</ul><p><img src="../fig/images2.png" alt="Flow Diagram for determining supvervised vs unsupervised" class="figure">.</p>
<p>(cite:<a href="https://developers.google.com/machine-learning/crash-course/overfitting/interpreting-loss-curves" class="external-link uri">https://developers.google.com/machine-learning/crash-course/overfitting/interpreting-loss-curves</a>)</p>
</div>
<div class="section level3">
<h3 id="underfitting">Underfitting<a class="anchor" aria-label="anchor" href="#underfitting"></a></h3>
<ul><li>Both training and validation loss stay high</li>
<li>Minimal improvement across epochs</li>
</ul><p>Symptoms: - Model isn’t learning patterns well</p>
<p>Solutions:</p>
<ul><li>Increase model complexity (more layers/units)</li>
<li>Train for more epochs</li>
<li>Try a lower learning rate</li>
<li>Change activation functions or optimizer</li>
</ul><p><img src="../fig/maxresdefault.jpg" alt="Flow Diagram for determining supvervised vs unsupervised" class="figure">.</p>
<p>(cite:<a href="https://developers.google.com/machine-learning/crash-course/overfitting/interpreting-loss-curves" class="external-link uri">https://developers.google.com/machine-learning/crash-course/overfitting/interpreting-loss-curves</a>)</p>
</div>
<div class="section level3">
<h3 id="high-variance-noisy-loss-curves">High Variance (Noisy Loss Curves)<a class="anchor" aria-label="anchor" href="#high-variance-noisy-loss-curves"></a></h3>
<p>Loss curves are erratic with spikes</p>
<p>Causes:</p>
<ul><li>Too high learning rate</li>
<li>Small batch size</li>
</ul><p>Insufficient data</p>
<p>Solutions:</p>
<ul><li>Reduce learning rate</li>
<li>Increase batch size</li>
<li>Collect more data</li>
</ul><p><img src="../fig/loss_curve.png" alt="Flow Diagram for determining supvervised vs unsupervised" class="figure">.</p>
</div>
</section><section><h2 class="section-heading" id="what-is-hyperparameter-tuning">What is Hyperparameter Tuning?<a class="anchor" aria-label="anchor" href="#what-is-hyperparameter-tuning"></a></h2>
<hr class="half-width"><p>Hyperparameters are settings defined before training that control the
learning process and model structure.</p>
<ul><li>They are not learned from the data</li>
<li>Must be set manually or via search strategies</li>
<li>Tuning them properly can dramatically boost model performance</li>
</ul><p><img src="../fig/common_hypers.png" alt="Flow Diagram for determining supvervised vs unsupervised" class="figure">.</p>
<p>As you can see the majority of machine learning models have some sort
of hyperparameter that can be configured. Usually when you go and use
these models they will have default parameters already coding into the
functions, allowing you to train a model before delevin into the actual
parameter refinement. Even then be prepared depending on your models
training time hyper parameter selection can be a long process, but there
are a few methods to speed it up.</p>
<p><img src="../fig/tuning_methods.png" alt="Flow Diagram for determining supvervised vs unsupervised" class="figure">.</p>
<p>Traditional machine learning usually has very few hyper parameters
compared to deep learning and therefore are normally quicker and easier.
Lets have a focus of a deep learning and see what hyper parameters are
required to run them.</p>
<p><img src="../fig/NN_hyper.png" alt="Flow Diagram for determining supvervised vs unsupervised" class="figure">.</p>
<p><img src="../fig/NN_hyper_refined.png" alt="Flow Diagram for determining supvervised vs unsupervised" class="figure">.</p>
<p>Lets focus on two important hyper parameters.</p>
<div class="section level3">
<h3 id="learning-rate">Learning rate<a class="anchor" aria-label="anchor" href="#learning-rate"></a></h3>
<p>The learning rate is one of the most critical hyperparameters in
training machine learning models, especially in gradient-based
optimization (like with neural networks). It controls how much the
model’s weights are updated during training in response to the
calculated loss gradient. Here’s how it affects training: If the
learning rate is too high:</p>
<ul><li>Overshooting: The model may skip over the optimal weights and never
converge, or even diverge completely.</li>
<li>Instability: The loss can fluctuate wildly and fail to
decrease.</li>
<li>Poor performance: May result in worse accuracy, even if the model
initially improves quickly.</li>
</ul><p>If the learning rate is too low:</p>
<ul><li>Slow convergence: Training takes a very long time, and you might
waste compute resources.</li>
<li>Getting stuck: The model can get trapped in local minima or plateaus
and not make meaningful progress.</li>
<li>Overfitting risk: More epochs might be needed, which can lead to
overfitting if regularization isn’t strong enough.</li>
</ul></div>
<div class="section level3">
<h3 id="optimizer">Optimizer<a class="anchor" aria-label="anchor" href="#optimizer"></a></h3>
<p>A machine learning optimizer is an algorithm used to adjust the
parameters (weights) of a machine learning model to minimize a loss
function during training.The purpose of an optimizer is to find the set
of model parameters that results in the best performance on your
training data, typically by minimizing error or maximizing accuracy.</p>
<p><img src="../fig/optimiser.png" alt="Flow Diagram for determining supvervised vs unsupervised" class="figure">.</p>
<p>A detailed understanding of how optimizers work is beyond the scope
of this course, so using SGD or Adam as a starting point is generally
recommended.</p>
</div>
<div class="section level3">
<h3 id="transfer-learning-the-hero-in-the-night">Transfer Learning (The hero in the night)<a class="anchor" aria-label="anchor" href="#transfer-learning-the-hero-in-the-night"></a></h3>
<p>Definition: Using a pre-trained model on a similar task and
fine-tuning it for your specific task.</p>
<p>How it Works:</p>
<ul><li>Take a model trained on a large dataset (e.g., ImageNet for
images)</li>
<li>Keep learned weights (often in lower layers)</li>
<li>Replace final layers for your task</li>
<li>Optionally, fine-tune some or all layers</li>
</ul><p>Benefits:</p>
<ul><li>Speeds up training</li>
<li>Requires less labeled data</li>
<li>Boosts performance on smaller datasets</li>
</ul></div>
</section><section><h2 class="section-heading" id="putting-it-all-together-examples">Putting it all together examples<a class="anchor" aria-label="anchor" href="#putting-it-all-together-examples"></a></h2>
<hr class="half-width"></section><section><h2 class="section-heading" id="classification-example">Classification example<a class="anchor" aria-label="anchor" href="#classification-example"></a></h2>
<hr class="half-width"><div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a><span class="im">import</span> torchvision</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose(</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a>    [transforms.ToTensor(),</span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a>     transforms.Normalize((<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>), (<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>))])</span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb18-9"><a href="#cb18-9" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" tabindex="-1"></a>trainset <span class="op">=</span> torchvision.datasets.CIFAR10(root<span class="op">=</span><span class="st">'./data'</span>, train<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb18-11"><a href="#cb18-11" tabindex="-1"></a>                                        download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform)</span>
<span id="cb18-12"><a href="#cb18-12" tabindex="-1"></a>trainloader <span class="op">=</span> torch.utils.data.DataLoader(trainset, batch_size<span class="op">=</span>batch_size,</span>
<span id="cb18-13"><a href="#cb18-13" tabindex="-1"></a>                                          shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb18-14"><a href="#cb18-14" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" tabindex="-1"></a>testset <span class="op">=</span> torchvision.datasets.CIFAR10(root<span class="op">=</span><span class="st">'./data'</span>, train<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb18-16"><a href="#cb18-16" tabindex="-1"></a>                                       download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform)</span>
<span id="cb18-17"><a href="#cb18-17" tabindex="-1"></a>testloader <span class="op">=</span> torch.utils.data.DataLoader(testset, batch_size<span class="op">=</span>batch_size,</span>
<span id="cb18-18"><a href="#cb18-18" tabindex="-1"></a>                                         shuffle<span class="op">=</span><span class="va">False</span>, num_workers<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb18-19"><a href="#cb18-19" tabindex="-1"></a></span>
<span id="cb18-20"><a href="#cb18-20" tabindex="-1"></a>classes <span class="op">=</span> (<span class="st">'plane'</span>, <span class="st">'car'</span>, <span class="st">'bird'</span>, <span class="st">'cat'</span>,</span>
<span id="cb18-21"><a href="#cb18-21" tabindex="-1"></a>           <span class="st">'deer'</span>, <span class="st">'dog'</span>, <span class="st">'frog'</span>, <span class="st">'horse'</span>, <span class="st">'ship'</span>, <span class="st">'truck'</span>)</span>
<span id="cb18-22"><a href="#cb18-22" tabindex="-1"></a></span>
<span id="cb18-23"><a href="#cb18-23" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb18-24"><a href="#cb18-24" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-25"><a href="#cb18-25" tabindex="-1"></a></span>
<span id="cb18-26"><a href="#cb18-26" tabindex="-1"></a><span class="kw">def</span> imshow(img):</span>
<span id="cb18-27"><a href="#cb18-27" tabindex="-1"></a>    img <span class="op">=</span> img <span class="op">/</span> <span class="dv">2</span> <span class="op">+</span> <span class="fl">0.5</span>     <span class="co"># unnormalize</span></span>
<span id="cb18-28"><a href="#cb18-28" tabindex="-1"></a>    npimg <span class="op">=</span> img.numpy()</span>
<span id="cb18-29"><a href="#cb18-29" tabindex="-1"></a>    plt.imshow(np.transpose(npimg, (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))</span>
<span id="cb18-30"><a href="#cb18-30" tabindex="-1"></a>    plt.show()</span>
<span id="cb18-31"><a href="#cb18-31" tabindex="-1"></a>    </span>
<span id="cb18-32"><a href="#cb18-32" tabindex="-1"></a>    <span class="co"># get some random training images</span></span>
<span id="cb18-33"><a href="#cb18-33" tabindex="-1"></a>dataiter <span class="op">=</span> <span class="bu">iter</span>(trainloader)</span>
<span id="cb18-34"><a href="#cb18-34" tabindex="-1"></a>images, labels <span class="op">=</span> <span class="bu">next</span>(dataiter)</span>
<span id="cb18-35"><a href="#cb18-35" tabindex="-1"></a></span>
<span id="cb18-36"><a href="#cb18-36" tabindex="-1"></a><span class="co"># show images</span></span>
<span id="cb18-37"><a href="#cb18-37" tabindex="-1"></a>imshow(torchvision.utils.make_grid(images))</span>
<span id="cb18-38"><a href="#cb18-38" tabindex="-1"></a><span class="co"># print labels</span></span>
<span id="cb18-39"><a href="#cb18-39" tabindex="-1"></a><span class="bu">print</span>(<span class="st">' '</span>.join(<span class="ss">f'</span><span class="sc">{</span>classes[labels[j]]<span class="sc">:5s}</span><span class="ss">'</span> <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(batch_size)))</span>
<span id="cb18-40"><a href="#cb18-40" tabindex="-1"></a></span>
<span id="cb18-41"><a href="#cb18-41" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb18-42"><a href="#cb18-42" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb18-43"><a href="#cb18-43" tabindex="-1"></a></span>
<span id="cb18-44"><a href="#cb18-44" tabindex="-1"></a></span>
<span id="cb18-45"><a href="#cb18-45" tabindex="-1"></a><span class="kw">class</span> Net(nn.Module):</span>
<span id="cb18-46"><a href="#cb18-46" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb18-47"><a href="#cb18-47" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb18-48"><a href="#cb18-48" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">5</span>)</span>
<span id="cb18-49"><a href="#cb18-49" tabindex="-1"></a>        <span class="va">self</span>.pool <span class="op">=</span> nn.MaxPool2d(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb18-50"><a href="#cb18-50" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Conv2d(<span class="dv">6</span>, <span class="dv">16</span>, <span class="dv">5</span>)</span>
<span id="cb18-51"><a href="#cb18-51" tabindex="-1"></a>        <span class="va">self</span>.fc1 <span class="op">=</span> nn.Linear(<span class="dv">16</span> <span class="op">*</span> <span class="dv">5</span> <span class="op">*</span> <span class="dv">5</span>, <span class="dv">120</span>)</span>
<span id="cb18-52"><a href="#cb18-52" tabindex="-1"></a>        <span class="va">self</span>.fc2 <span class="op">=</span> nn.Linear(<span class="dv">120</span>, <span class="dv">84</span>)</span>
<span id="cb18-53"><a href="#cb18-53" tabindex="-1"></a>        <span class="va">self</span>.fc3 <span class="op">=</span> nn.Linear(<span class="dv">84</span>, <span class="dv">10</span>)</span>
<span id="cb18-54"><a href="#cb18-54" tabindex="-1"></a></span>
<span id="cb18-55"><a href="#cb18-55" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb18-56"><a href="#cb18-56" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(F.relu(<span class="va">self</span>.conv1(x)))</span>
<span id="cb18-57"><a href="#cb18-57" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pool(F.relu(<span class="va">self</span>.conv2(x)))</span>
<span id="cb18-58"><a href="#cb18-58" tabindex="-1"></a>        x <span class="op">=</span> torch.flatten(x, <span class="dv">1</span>) <span class="co"># flatten all dimensions except batch</span></span>
<span id="cb18-59"><a href="#cb18-59" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc1(x))</span>
<span id="cb18-60"><a href="#cb18-60" tabindex="-1"></a>        x <span class="op">=</span> F.relu(<span class="va">self</span>.fc2(x))</span>
<span id="cb18-61"><a href="#cb18-61" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc3(x)</span>
<span id="cb18-62"><a href="#cb18-62" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb18-63"><a href="#cb18-63" tabindex="-1"></a></span>
<span id="cb18-64"><a href="#cb18-64" tabindex="-1"></a></span>
<span id="cb18-65"><a href="#cb18-65" tabindex="-1"></a>net <span class="op">=</span> Net()</span>
<span id="cb18-66"><a href="#cb18-66" tabindex="-1"></a></span>
<span id="cb18-67"><a href="#cb18-67" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb18-68"><a href="#cb18-68" tabindex="-1"></a></span>
<span id="cb18-69"><a href="#cb18-69" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb18-70"><a href="#cb18-70" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(net.parameters(), lr<span class="op">=</span><span class="fl">0.001</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb18-71"><a href="#cb18-71" tabindex="-1"></a></span>
<span id="cb18-72"><a href="#cb18-72" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):  <span class="co"># loop over the dataset multiple times</span></span>
<span id="cb18-73"><a href="#cb18-73" tabindex="-1"></a></span>
<span id="cb18-74"><a href="#cb18-74" tabindex="-1"></a>    running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb18-75"><a href="#cb18-75" tabindex="-1"></a>    <span class="cf">for</span> i, data <span class="kw">in</span> <span class="bu">enumerate</span>(trainloader, <span class="dv">0</span>):</span>
<span id="cb18-76"><a href="#cb18-76" tabindex="-1"></a>        <span class="co"># get the inputs; data is a list of [inputs, labels]</span></span>
<span id="cb18-77"><a href="#cb18-77" tabindex="-1"></a>        inputs, labels <span class="op">=</span> data</span>
<span id="cb18-78"><a href="#cb18-78" tabindex="-1"></a></span>
<span id="cb18-79"><a href="#cb18-79" tabindex="-1"></a>        <span class="co"># zero the parameter gradients</span></span>
<span id="cb18-80"><a href="#cb18-80" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb18-81"><a href="#cb18-81" tabindex="-1"></a></span>
<span id="cb18-82"><a href="#cb18-82" tabindex="-1"></a>        <span class="co"># forward + backward + optimize</span></span>
<span id="cb18-83"><a href="#cb18-83" tabindex="-1"></a>        outputs <span class="op">=</span> net(inputs)</span>
<span id="cb18-84"><a href="#cb18-84" tabindex="-1"></a>        loss <span class="op">=</span> criterion(outputs, labels)</span>
<span id="cb18-85"><a href="#cb18-85" tabindex="-1"></a>        loss.backward()</span>
<span id="cb18-86"><a href="#cb18-86" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb18-87"><a href="#cb18-87" tabindex="-1"></a></span>
<span id="cb18-88"><a href="#cb18-88" tabindex="-1"></a>        <span class="co"># print statistics</span></span>
<span id="cb18-89"><a href="#cb18-89" tabindex="-1"></a>        running_loss <span class="op">+=</span> loss.item()</span>
<span id="cb18-90"><a href="#cb18-90" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">2000</span> <span class="op">==</span> <span class="dv">1999</span>:    <span class="co"># print every 2000 mini-batches</span></span>
<span id="cb18-91"><a href="#cb18-91" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f'[</span><span class="sc">{</span>epoch <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>i <span class="op">+</span> <span class="dv">1</span><span class="sc">:5d}</span><span class="ss">] loss: </span><span class="sc">{</span>running_loss <span class="op">/</span> <span class="dv">2000</span><span class="sc">:.3f}</span><span class="ss">'</span>)</span>
<span id="cb18-92"><a href="#cb18-92" tabindex="-1"></a>            running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb18-93"><a href="#cb18-93" tabindex="-1"></a></span>
<span id="cb18-94"><a href="#cb18-94" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Finished Training'</span>)</span>
<span id="cb18-95"><a href="#cb18-95" tabindex="-1"></a></span>
<span id="cb18-96"><a href="#cb18-96" tabindex="-1"></a>dataiter <span class="op">=</span> <span class="bu">iter</span>(testloader)</span>
<span id="cb18-97"><a href="#cb18-97" tabindex="-1"></a>images, labels <span class="op">=</span> <span class="bu">next</span>(dataiter)</span>
<span id="cb18-98"><a href="#cb18-98" tabindex="-1"></a></span>
<span id="cb18-99"><a href="#cb18-99" tabindex="-1"></a><span class="co"># print images</span></span>
<span id="cb18-100"><a href="#cb18-100" tabindex="-1"></a>imshow(torchvision.utils.make_grid(images))</span>
<span id="cb18-101"><a href="#cb18-101" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'GroundTruth: '</span>, <span class="st">' '</span>.join(<span class="ss">f'</span><span class="sc">{</span>classes[labels[j]]<span class="sc">:5s}</span><span class="ss">'</span> <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>)))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>car   deer  bird  bird
[1,  2000] loss: 2.221
[1,  4000] loss: 1.841
[1,  6000] loss: 1.684
[1,  8000] loss: 1.575
[1, 10000] loss: 1.522
[1, 12000] loss: 1.490
[2,  2000] loss: 1.422
[2,  4000] loss: 1.401
[2,  6000] loss: 1.341
[2,  8000] loss: 1.356
[2, 10000] loss: 1.317
[2, 12000] loss: 1.308
[3,  2000] loss: 1.253
[3,  4000] loss: 1.232
[3,  6000] loss: 1.228
[3,  8000] loss: 1.194
[3, 10000] loss: 1.206
[3, 12000] loss: 1.205
[4,  2000] loss: 1.125
[4,  4000] loss: 1.118
[4,  6000] loss: 1.125
[4,  8000] loss: 1.151
[4, 10000] loss: 1.128
[4, 12000] loss: 1.114
[5,  2000] loss: 1.051
[5,  4000] loss: 1.053
[5,  6000] loss: 1.069
[5,  8000] loss: 1.047
[5, 10000] loss: 1.057
[5, 12000] loss: 1.073
Finished Training
GroundTruth:  cat   ship  ship  plane
Predicted:  cat   ship  ship  ship
Accuracy of the network on the 10000 test images: 59 %
</code></pre>
</div>
</section><section><h2 class="section-heading" id="time-series-parameter-example">Time series parameter example<a class="anchor" aria-label="anchor" href="#time-series-parameter-example"></a></h2>
<hr class="half-width"><p>data: <a href="https://drive.google.com/file/d/1CkL0_PEZwMFHz97sExRBluh_CYxwKqFr/view?usp=sharing" class="external-link uri">https://drive.google.com/file/d/1CkL0_PEZwMFHz97sExRBluh_CYxwKqFr/view?usp=sharing</a></p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb20-6"><a href="#cb20-6" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb20-7"><a href="#cb20-7" tabindex="-1"></a><span class="im">import</span> torch.utils.data <span class="im">as</span> data</span>
<span id="cb20-8"><a href="#cb20-8" tabindex="-1"></a> </span>
<span id="cb20-9"><a href="#cb20-9" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'AirPassengers.csv'</span>)</span>
<span id="cb20-10"><a href="#cb20-10" tabindex="-1"></a>timeseries <span class="op">=</span> df[[<span class="st">"Passengers"</span>]].values.astype(<span class="st">'float32'</span>)</span>
<span id="cb20-11"><a href="#cb20-11" tabindex="-1"></a> </span>
<span id="cb20-12"><a href="#cb20-12" tabindex="-1"></a><span class="co"># train-test split for time series</span></span>
<span id="cb20-13"><a href="#cb20-13" tabindex="-1"></a>train_size <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(timeseries) <span class="op">*</span> <span class="fl">0.67</span>)</span>
<span id="cb20-14"><a href="#cb20-14" tabindex="-1"></a>test_size <span class="op">=</span> <span class="bu">len</span>(timeseries) <span class="op">-</span> train_size</span>
<span id="cb20-15"><a href="#cb20-15" tabindex="-1"></a>train, test <span class="op">=</span> timeseries[:train_size], timeseries[train_size:]</span>
<span id="cb20-16"><a href="#cb20-16" tabindex="-1"></a> </span>
<span id="cb20-17"><a href="#cb20-17" tabindex="-1"></a><span class="kw">def</span> create_dataset(dataset, lookback):</span>
<span id="cb20-18"><a href="#cb20-18" tabindex="-1"></a>    <span class="co">"""Transform a time series into a prediction dataset</span></span>
<span id="cb20-19"><a href="#cb20-19" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb20-20"><a href="#cb20-20" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb20-21"><a href="#cb20-21" tabindex="-1"></a><span class="co">        dataset: A numpy array of time series, first dimension is the time steps</span></span>
<span id="cb20-22"><a href="#cb20-22" tabindex="-1"></a><span class="co">        lookback: Size of window for prediction</span></span>
<span id="cb20-23"><a href="#cb20-23" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb20-24"><a href="#cb20-24" tabindex="-1"></a>    X, y <span class="op">=</span> [], []</span>
<span id="cb20-25"><a href="#cb20-25" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(dataset)<span class="op">-</span>lookback):</span>
<span id="cb20-26"><a href="#cb20-26" tabindex="-1"></a>        feature <span class="op">=</span> dataset[i:i<span class="op">+</span>lookback]</span>
<span id="cb20-27"><a href="#cb20-27" tabindex="-1"></a>        target <span class="op">=</span> dataset[i<span class="op">+</span><span class="dv">1</span>:i<span class="op">+</span>lookback<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb20-28"><a href="#cb20-28" tabindex="-1"></a>        X.append(feature)</span>
<span id="cb20-29"><a href="#cb20-29" tabindex="-1"></a>        y.append(target)</span>
<span id="cb20-30"><a href="#cb20-30" tabindex="-1"></a>    <span class="cf">return</span> torch.tensor(X), torch.tensor(y)</span>
<span id="cb20-31"><a href="#cb20-31" tabindex="-1"></a> </span>
<span id="cb20-32"><a href="#cb20-32" tabindex="-1"></a>lookback <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb20-33"><a href="#cb20-33" tabindex="-1"></a>X_train, y_train <span class="op">=</span> create_dataset(train, lookback<span class="op">=</span>lookback)</span>
<span id="cb20-34"><a href="#cb20-34" tabindex="-1"></a>X_test, y_test <span class="op">=</span> create_dataset(test, lookback<span class="op">=</span>lookback)</span>
<span id="cb20-35"><a href="#cb20-35" tabindex="-1"></a> </span>
<span id="cb20-36"><a href="#cb20-36" tabindex="-1"></a><span class="kw">class</span> AirModel(nn.Module):</span>
<span id="cb20-37"><a href="#cb20-37" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb20-38"><a href="#cb20-38" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb20-39"><a href="#cb20-39" tabindex="-1"></a>        <span class="va">self</span>.lstm <span class="op">=</span> nn.LSTM(input_size<span class="op">=</span><span class="dv">1</span>, hidden_size<span class="op">=</span><span class="dv">50</span>, num_layers<span class="op">=</span><span class="dv">1</span>, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-40"><a href="#cb20-40" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> nn.Linear(<span class="dv">50</span>, <span class="dv">1</span>)</span>
<span id="cb20-41"><a href="#cb20-41" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb20-42"><a href="#cb20-42" tabindex="-1"></a>        x, _ <span class="op">=</span> <span class="va">self</span>.lstm(x)</span>
<span id="cb20-43"><a href="#cb20-43" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear(x)</span>
<span id="cb20-44"><a href="#cb20-44" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb20-45"><a href="#cb20-45" tabindex="-1"></a> </span>
<span id="cb20-46"><a href="#cb20-46" tabindex="-1"></a>model <span class="op">=</span> AirModel()</span>
<span id="cb20-47"><a href="#cb20-47" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters())</span>
<span id="cb20-48"><a href="#cb20-48" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.MSELoss()</span>
<span id="cb20-49"><a href="#cb20-49" tabindex="-1"></a>loader <span class="op">=</span> data.DataLoader(data.TensorDataset(X_train, y_train), shuffle<span class="op">=</span><span class="va">True</span>, batch_size<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb20-50"><a href="#cb20-50" tabindex="-1"></a> </span>
<span id="cb20-51"><a href="#cb20-51" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb20-52"><a href="#cb20-52" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb20-53"><a href="#cb20-53" tabindex="-1"></a>    model.train()</span>
<span id="cb20-54"><a href="#cb20-54" tabindex="-1"></a>    <span class="cf">for</span> X_batch, y_batch <span class="kw">in</span> loader:</span>
<span id="cb20-55"><a href="#cb20-55" tabindex="-1"></a>        y_pred <span class="op">=</span> model(X_batch)</span>
<span id="cb20-56"><a href="#cb20-56" tabindex="-1"></a>        loss <span class="op">=</span> loss_fn(y_pred, y_batch)</span>
<span id="cb20-57"><a href="#cb20-57" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb20-58"><a href="#cb20-58" tabindex="-1"></a>        loss.backward()</span>
<span id="cb20-59"><a href="#cb20-59" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb20-60"><a href="#cb20-60" tabindex="-1"></a>    <span class="co"># Validation</span></span>
<span id="cb20-61"><a href="#cb20-61" tabindex="-1"></a>    <span class="cf">if</span> epoch <span class="op">%</span> <span class="dv">100</span> <span class="op">!=</span> <span class="dv">0</span>:</span>
<span id="cb20-62"><a href="#cb20-62" tabindex="-1"></a>        <span class="cf">continue</span></span>
<span id="cb20-63"><a href="#cb20-63" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb20-64"><a href="#cb20-64" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb20-65"><a href="#cb20-65" tabindex="-1"></a>        y_pred <span class="op">=</span> model(X_train)</span>
<span id="cb20-66"><a href="#cb20-66" tabindex="-1"></a>        train_rmse <span class="op">=</span> np.sqrt(loss_fn(y_pred, y_train))</span>
<span id="cb20-67"><a href="#cb20-67" tabindex="-1"></a>        y_pred <span class="op">=</span> model(X_test)</span>
<span id="cb20-68"><a href="#cb20-68" tabindex="-1"></a>        test_rmse <span class="op">=</span> np.sqrt(loss_fn(y_pred, y_test))</span>
<span id="cb20-69"><a href="#cb20-69" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Epoch </span><span class="sc">%d</span><span class="st">: train RMSE </span><span class="sc">%.4f</span><span class="st">, test RMSE </span><span class="sc">%.4f</span><span class="st">"</span> <span class="op">%</span> (epoch, train_rmse, test_rmse))</span>
<span id="cb20-70"><a href="#cb20-70" tabindex="-1"></a> </span>
<span id="cb20-71"><a href="#cb20-71" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb20-72"><a href="#cb20-72" tabindex="-1"></a>    <span class="co"># shift train predictions for plotting</span></span>
<span id="cb20-73"><a href="#cb20-73" tabindex="-1"></a>    train_plot <span class="op">=</span> np.ones_like(timeseries) <span class="op">*</span> np.nan</span>
<span id="cb20-74"><a href="#cb20-74" tabindex="-1"></a>    y_pred <span class="op">=</span> model(X_train)</span>
<span id="cb20-75"><a href="#cb20-75" tabindex="-1"></a>    y_pred <span class="op">=</span> y_pred[:, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb20-76"><a href="#cb20-76" tabindex="-1"></a>    train_plot[lookback:train_size] <span class="op">=</span> model(X_train)[:, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb20-77"><a href="#cb20-77" tabindex="-1"></a>    <span class="co"># shift test predictions for plotting</span></span>
<span id="cb20-78"><a href="#cb20-78" tabindex="-1"></a>    test_plot <span class="op">=</span> np.ones_like(timeseries) <span class="op">*</span> np.nan</span>
<span id="cb20-79"><a href="#cb20-79" tabindex="-1"></a>    test_plot[train_size<span class="op">+</span>lookback:<span class="bu">len</span>(timeseries)] <span class="op">=</span> model(X_test)[:, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb20-80"><a href="#cb20-80" tabindex="-1"></a><span class="co"># plot</span></span>
<span id="cb20-81"><a href="#cb20-81" tabindex="-1"></a>plt.plot(timeseries)</span>
<span id="cb20-82"><a href="#cb20-82" tabindex="-1"></a>plt.plot(train_plot, c<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb20-83"><a href="#cb20-83" tabindex="-1"></a>plt.plot(test_plot, c<span class="op">=</span><span class="st">'g'</span>)</span>
<span id="cb20-84"><a href="#cb20-84" tabindex="-1"></a>plt.show()</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>/home/corcor27/Documents/time_series_example.py:66: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)
  train_rmse = np.sqrt(loss_fn(y_pred, y_train))
/home/corcor27/Documents/time_series_example.py:68: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)
  test_rmse = np.sqrt(loss_fn(y_pred, y_test))
Epoch 0: train RMSE 226.0555, test RMSE 425.2918
Epoch 100: train RMSE 180.3027, test RMSE 377.3510
Epoch 200: train RMSE 145.4659, test RMSE 339.4402
Epoch 300: train RMSE 116.3985, test RMSE 305.5107
Epoch 400: train RMSE 94.1039, test RMSE 275.5995
Epoch 500: train RMSE 72.6241, test RMSE 244.1789
Epoch 600: train RMSE 56.9958, test RMSE 215.7754
Epoch 700: train RMSE 45.2195, test RMSE 190.4163
Epoch 800: train RMSE 37.3441, test RMSE 168.7669
Epoch 900: train RMSE 31.8818, test RMSE 150.3023
Epoch 1000: train RMSE 28.1362, test RMSE 134.6788
Epoch 1100: train RMSE 25.6366, test RMSE 121.9030
Epoch 1200: train RMSE 24.1227, test RMSE 110.8427
Epoch 1300: train RMSE 23.1641, test RMSE 102.8851
Epoch 1400: train RMSE 22.0363, test RMSE 96.8792
Epoch 1500: train RMSE 21.3094, test RMSE 90.6086
Epoch 1600: train RMSE 20.6500, test RMSE 86.9244
Epoch 1700: train RMSE 20.8895, test RMSE 83.7368
Epoch 1800: train RMSE 20.1709, test RMSE 82.1835
Epoch 1900: train RMSE 19.8376, test RMSE 80.0266
/home/corcor27/Documents/time_series_example.py:76: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.
  train_plot[lookback:train_size] = model(X_train)[:, -1, :]
/home/corcor27/Documents/time_series_example.py:79: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments.
  test_plot[train_size+lookback:len(timeseries)] = model(X_test)[:, -1, :]
</code></pre>
</div>
</section><section><h2 class="section-heading" id="parameter-estimation-example">Parameter estimation example<a class="anchor" aria-label="anchor" href="#parameter-estimation-example"></a></h2>
<hr class="half-width"><div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb22-4"><a href="#cb22-4" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb22-5"><a href="#cb22-5" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb22-6"><a href="#cb22-6" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb22-7"><a href="#cb22-7" tabindex="-1"></a><span class="im">import</span> tqdm</span>
<span id="cb22-8"><a href="#cb22-8" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb22-9"><a href="#cb22-9" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_california_housing</span>
<span id="cb22-10"><a href="#cb22-10" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb22-11"><a href="#cb22-11" tabindex="-1"></a> </span>
<span id="cb22-12"><a href="#cb22-12" tabindex="-1"></a><span class="co"># Read data</span></span>
<span id="cb22-13"><a href="#cb22-13" tabindex="-1"></a>data <span class="op">=</span> fetch_california_housing()</span>
<span id="cb22-14"><a href="#cb22-14" tabindex="-1"></a>X, y <span class="op">=</span> data.data, data.target</span>
<span id="cb22-15"><a href="#cb22-15" tabindex="-1"></a> </span>
<span id="cb22-16"><a href="#cb22-16" tabindex="-1"></a><span class="co"># train-test split for model evaluation</span></span>
<span id="cb22-17"><a href="#cb22-17" tabindex="-1"></a>X_train_raw, X_test_raw, y_train, y_test <span class="op">=</span> train_test_split(X, y, train_size<span class="op">=</span><span class="fl">0.7</span>, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb22-18"><a href="#cb22-18" tabindex="-1"></a> </span>
<span id="cb22-19"><a href="#cb22-19" tabindex="-1"></a><span class="co"># Standardizing data</span></span>
<span id="cb22-20"><a href="#cb22-20" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb22-21"><a href="#cb22-21" tabindex="-1"></a>scaler.fit(X_train_raw)</span>
<span id="cb22-22"><a href="#cb22-22" tabindex="-1"></a>X_train <span class="op">=</span> scaler.transform(X_train_raw)</span>
<span id="cb22-23"><a href="#cb22-23" tabindex="-1"></a>X_test <span class="op">=</span> scaler.transform(X_test_raw)</span>
<span id="cb22-24"><a href="#cb22-24" tabindex="-1"></a> </span>
<span id="cb22-25"><a href="#cb22-25" tabindex="-1"></a><span class="co"># Convert to 2D PyTorch tensors</span></span>
<span id="cb22-26"><a href="#cb22-26" tabindex="-1"></a>X_train <span class="op">=</span> torch.tensor(X_train, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb22-27"><a href="#cb22-27" tabindex="-1"></a>y_train <span class="op">=</span> torch.tensor(y_train, dtype<span class="op">=</span>torch.float32).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb22-28"><a href="#cb22-28" tabindex="-1"></a>X_test <span class="op">=</span> torch.tensor(X_test, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb22-29"><a href="#cb22-29" tabindex="-1"></a>y_test <span class="op">=</span> torch.tensor(y_test, dtype<span class="op">=</span>torch.float32).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb22-30"><a href="#cb22-30" tabindex="-1"></a> </span>
<span id="cb22-31"><a href="#cb22-31" tabindex="-1"></a><span class="co"># Define the model</span></span>
<span id="cb22-32"><a href="#cb22-32" tabindex="-1"></a>model <span class="op">=</span> nn.Sequential(</span>
<span id="cb22-33"><a href="#cb22-33" tabindex="-1"></a>    nn.Linear(<span class="dv">8</span>, <span class="dv">24</span>),</span>
<span id="cb22-34"><a href="#cb22-34" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb22-35"><a href="#cb22-35" tabindex="-1"></a>    nn.Linear(<span class="dv">24</span>, <span class="dv">12</span>),</span>
<span id="cb22-36"><a href="#cb22-36" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb22-37"><a href="#cb22-37" tabindex="-1"></a>    nn.Linear(<span class="dv">12</span>, <span class="dv">6</span>),</span>
<span id="cb22-38"><a href="#cb22-38" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb22-39"><a href="#cb22-39" tabindex="-1"></a>    nn.Linear(<span class="dv">6</span>, <span class="dv">1</span>)</span>
<span id="cb22-40"><a href="#cb22-40" tabindex="-1"></a>)</span>
<span id="cb22-41"><a href="#cb22-41" tabindex="-1"></a> </span>
<span id="cb22-42"><a href="#cb22-42" tabindex="-1"></a><span class="co"># loss function and optimizer</span></span>
<span id="cb22-43"><a href="#cb22-43" tabindex="-1"></a>loss_fn <span class="op">=</span> nn.MSELoss()  <span class="co"># mean square error</span></span>
<span id="cb22-44"><a href="#cb22-44" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">0.0001</span>)</span>
<span id="cb22-45"><a href="#cb22-45" tabindex="-1"></a> </span>
<span id="cb22-46"><a href="#cb22-46" tabindex="-1"></a>n_epochs <span class="op">=</span> <span class="dv">20</span>   <span class="co"># number of epochs to run</span></span>
<span id="cb22-47"><a href="#cb22-47" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">10</span>  <span class="co"># size of each batch</span></span>
<span id="cb22-48"><a href="#cb22-48" tabindex="-1"></a>batch_start <span class="op">=</span> torch.arange(<span class="dv">0</span>, <span class="bu">len</span>(X_train), batch_size)</span>
<span id="cb22-49"><a href="#cb22-49" tabindex="-1"></a> </span>
<span id="cb22-50"><a href="#cb22-50" tabindex="-1"></a><span class="co"># Hold the best model</span></span>
<span id="cb22-51"><a href="#cb22-51" tabindex="-1"></a>best_mse <span class="op">=</span> np.inf   <span class="co"># init to infinity</span></span>
<span id="cb22-52"><a href="#cb22-52" tabindex="-1"></a>best_weights <span class="op">=</span> <span class="va">None</span></span>
<span id="cb22-53"><a href="#cb22-53" tabindex="-1"></a>history <span class="op">=</span> []</span>
<span id="cb22-54"><a href="#cb22-54" tabindex="-1"></a> </span>
<span id="cb22-55"><a href="#cb22-55" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb22-56"><a href="#cb22-56" tabindex="-1"></a>    model.train()</span>
<span id="cb22-57"><a href="#cb22-57" tabindex="-1"></a>    <span class="bu">print</span>(epoch)</span>
<span id="cb22-58"><a href="#cb22-58" tabindex="-1"></a>    <span class="cf">with</span> tqdm.tqdm(batch_start, unit<span class="op">=</span><span class="st">"batch"</span>, mininterval<span class="op">=</span><span class="dv">0</span>, disable<span class="op">=</span><span class="va">True</span>) <span class="im">as</span> bar:</span>
<span id="cb22-59"><a href="#cb22-59" tabindex="-1"></a>        bar.set_description(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb22-60"><a href="#cb22-60" tabindex="-1"></a>        <span class="cf">for</span> start <span class="kw">in</span> bar:</span>
<span id="cb22-61"><a href="#cb22-61" tabindex="-1"></a>            <span class="co"># take a batch</span></span>
<span id="cb22-62"><a href="#cb22-62" tabindex="-1"></a>            X_batch <span class="op">=</span> X_train[start:start<span class="op">+</span>batch_size]</span>
<span id="cb22-63"><a href="#cb22-63" tabindex="-1"></a>            y_batch <span class="op">=</span> y_train[start:start<span class="op">+</span>batch_size]</span>
<span id="cb22-64"><a href="#cb22-64" tabindex="-1"></a>            <span class="co"># forward pass</span></span>
<span id="cb22-65"><a href="#cb22-65" tabindex="-1"></a>            y_pred <span class="op">=</span> model(X_batch)</span>
<span id="cb22-66"><a href="#cb22-66" tabindex="-1"></a>            loss <span class="op">=</span> loss_fn(y_pred, y_batch)</span>
<span id="cb22-67"><a href="#cb22-67" tabindex="-1"></a>            <span class="co"># backward pass</span></span>
<span id="cb22-68"><a href="#cb22-68" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="cb22-69"><a href="#cb22-69" tabindex="-1"></a>            loss.backward()</span>
<span id="cb22-70"><a href="#cb22-70" tabindex="-1"></a>            <span class="co"># update weights</span></span>
<span id="cb22-71"><a href="#cb22-71" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb22-72"><a href="#cb22-72" tabindex="-1"></a>            <span class="co"># print progress</span></span>
<span id="cb22-73"><a href="#cb22-73" tabindex="-1"></a>            bar.set_postfix(mse<span class="op">=</span><span class="bu">float</span>(loss))</span>
<span id="cb22-74"><a href="#cb22-74" tabindex="-1"></a>    <span class="co"># evaluate accuracy at end of each epoch</span></span>
<span id="cb22-75"><a href="#cb22-75" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb22-76"><a href="#cb22-76" tabindex="-1"></a>    y_pred <span class="op">=</span> model(X_test)</span>
<span id="cb22-77"><a href="#cb22-77" tabindex="-1"></a>    mse <span class="op">=</span> loss_fn(y_pred, y_test)</span>
<span id="cb22-78"><a href="#cb22-78" tabindex="-1"></a>    mse <span class="op">=</span> <span class="bu">float</span>(mse)</span>
<span id="cb22-79"><a href="#cb22-79" tabindex="-1"></a>    history.append(mse)</span>
<span id="cb22-80"><a href="#cb22-80" tabindex="-1"></a>    <span class="cf">if</span> mse <span class="op">&lt;</span> best_mse:</span>
<span id="cb22-81"><a href="#cb22-81" tabindex="-1"></a>        best_mse <span class="op">=</span> mse</span>
<span id="cb22-82"><a href="#cb22-82" tabindex="-1"></a> </span>
<span id="cb22-83"><a href="#cb22-83" tabindex="-1"></a><span class="co"># restore model and return best accuracy</span></span>
<span id="cb22-84"><a href="#cb22-84" tabindex="-1"></a><span class="co">#model.load_state_dict(best_weights)</span></span>
<span id="cb22-85"><a href="#cb22-85" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"MSE: </span><span class="sc">%.2f</span><span class="st">"</span> <span class="op">%</span> best_mse)</span>
<span id="cb22-86"><a href="#cb22-86" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"RMSE: </span><span class="sc">%.2f</span><span class="st">"</span> <span class="op">%</span> np.sqrt(best_mse))</span>
<span id="cb22-87"><a href="#cb22-87" tabindex="-1"></a>plt.plot(history)</span>
<span id="cb22-88"><a href="#cb22-88" tabindex="-1"></a>plt.show()</span>
<span id="cb22-89"><a href="#cb22-89" tabindex="-1"></a> </span>
<span id="cb22-90"><a href="#cb22-90" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb22-91"><a href="#cb22-91" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb22-92"><a href="#cb22-92" tabindex="-1"></a>    <span class="co"># Test out inference with 5 samples</span></span>
<span id="cb22-93"><a href="#cb22-93" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>):</span>
<span id="cb22-94"><a href="#cb22-94" tabindex="-1"></a>        X_sample <span class="op">=</span> X_test_raw[i: i<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb22-95"><a href="#cb22-95" tabindex="-1"></a>        X_sample <span class="op">=</span> scaler.transform(X_sample)</span>
<span id="cb22-96"><a href="#cb22-96" tabindex="-1"></a>        X_sample <span class="op">=</span> torch.tensor(X_sample, dtype<span class="op">=</span>torch.float32)</span>
<span id="cb22-97"><a href="#cb22-97" tabindex="-1"></a>        y_pred <span class="op">=</span> model(X_sample)</span>
<span id="cb22-98"><a href="#cb22-98" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>X_test_raw[i]<span class="sc">}</span><span class="ss"> -&gt; </span><span class="sc">{</span>y_pred[<span class="dv">0</span>]<span class="sc">.</span>numpy()<span class="sc">}</span><span class="ss"> (expected </span><span class="sc">{</span>y_test[i]<span class="sc">.</span>numpy()<span class="sc">}</span><span class="ss">)"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
MSE: 0.39
RMSE: 0.63
[   3.3214       47.            4.91287879    1.10606061  704.
    2.66666667   33.96       -118.32      ] -&gt; [2.4658601] (expected [1.665])
[   2.2019       24.            4.96732026    1.09477124  882.
    2.88235294   37.07       -120.84      ] -&gt; [1.0679097] (expected [1.])
[   3.5128       23.            5.17777778    0.95061728  911.
    2.24938272   36.74       -119.69      ] -&gt; [1.26838] (expected [1.216])
[ 5.70180000e+00  2.40000000e+01  6.49022556e+00  1.01954887e+00
  2.28600000e+03  3.43759398e+00  3.38300000e+01 -1.18050000e+02] -&gt; [2.6457143] (expected [2.867])
[   3.5781       35.            6.24624625    1.0960961  1026.
    3.08108108   36.62       -119.73      ] -&gt; [1.273118] (expected [0.928])</code></pre>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</section></div> <!-- / div.lesson-content -->
    </main><!-- / main#main-content.main-content --><nav class="bottom-pagination mx-md-4" aria-label="Previous and Next Chapter"><div class="d-block d-sm-block d-md-none">
        <a class="chapter-link" href="../instructor/04-model-selection%202.html"><i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>Previous</a>
        <a class="chapter-link float-end" href="../instructor/06-model-evaluation.html">Next<i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i></a>
      </div>
      <!-- content for large screens -->
      <div class="d-none d-sm-none d-md-block">
        <a class="chapter-link" href="../instructor/04-model-selection%202.html" rel="prev">
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-left"></i>
          Previous: Machine learning:
        </a>
        <a class="chapter-link float-end" href="../instructor/06-model-evaluation.html" rel="next">
          Next: Model Evaluation...
          <i aria-hidden="true" class="small-arrow" data-feather="arrow-right"></i>
        </a>
      </div>
    </nav></div> <!-- / div.primary-content.col-xs-12 -->
<!-- END:   inst/pkgdown/templates/content-instructor.html-->

      </div><!--/div.row-->
      		<footer class="row footer mx-md-3"><hr><div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/RSEjas140/AAISS25_html/edit/main/episodes/05-training.Rmd" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/RSEjas140/AAISS25_html/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/RSEjas140/AAISS25_html/" class="external-link">Source</a></p>
				<p><a href="https://github.com/RSEjas140/AAISS25_html/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:jas140@aber.ac.uk">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/d03c89cd0fa3931a82df26aa15388efab7cf4e9b" class="external-link">sandpaper (0.16.13.9000)</a>, <a href="https://github.com/carpentries/pegboard/tree/5e0d711121a581777d83cf270e298c933f112d52" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.6" class="external-link">varnish (1.0.6)</a></p>
			</div>
		</footer></div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "LearningResource",
  "@id": "https://RSEjas140.github.io/AAISS25_html/instructor/05-training.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/LearningResource/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "Machine learning: model training",
  "creativeWorkStatus": "active",
  "url": "https://RSEjas140.github.io/AAISS25_html/instructor/05-training.html",
  "identifier": "https://RSEjas140.github.io/AAISS25_html/instructor/05-training.html",
  "dateCreated": "2025-07-04",
  "dateModified": "2025-07-09",
  "datePublished": "2025-07-09"
}

  </script><script>
		feather.replace();
	</script></body></html><!-- END:   inst/pkgdown/templates/layout.html-->

